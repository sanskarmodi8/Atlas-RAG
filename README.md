---
title: AtlasRAG Backend
emoji: ğŸ“š
colorFrom: blue
colorTo: indigo
sdk: docker
app_port: 7860
pinned: false
license: mit
---

# AtlasRAG â€” Hybrid Graph-Enhanced Retrieval-Augmented Generation System

AtlasRAG is a **document-centric Question Answering and Summarization system** that combines **vector search**, **symbolic graph reasoning**, and **LLM-based generation** to produce **grounded, citation-backed answers** from uploaded documents.

The project demonstrates **end-to-end RAG system engineering**, covering ingestion, retrieval design, reranking, citation grounding, evaluation, memory, and real-world deployment.

---

## ğŸŒ Live Deployment

AtlasRAG is fully deployed and publicly accessible:

* **Frontend (Vercel)**
  ğŸ‘‰ [https://atlas-rag.vercel.app/](https://atlas-rag.vercel.app/)

* **Backend API (Hugging Face Spaces)**
  ğŸ‘‰ [https://sanskarmodi-atlasrag-backend.hf.space/](https://sanskarmodi-atlasrag-backend.hf.space/)

The frontend communicates with the deployed backend over REST APIs.

---

## ğŸ–¥ï¸ Application Preview

**Home Page (Document Upload & Chat Interface):**

![AtlasRAG Web App Screenshot](https://drive.google.com/uc?id=1BIfz53BOlS5W9LmHc66sBGyZLO9tg83j)

---

## âœ¨ Key Features

* **Document Upload & Parsing**

  * PDF ingestion
  * Page-aware chunking with metadata
* **Hybrid Retrieval (Core Contribution)**

  * Dense vector search (semantic similarity)
  * Sparse lexical search (BM25)
  * Concept co-occurrence graph
  * Hybrid Graph-RAG retrieval pipeline
* **Citation-Backed QA**

  * Answers grounded strictly in retrieved chunks
  * Page-level citations extracted post-generation
* **Document Summarization**

  * Full-document summarization using all indexed chunks
* **Conversation Memory**

  * Short-term session-based conversation history
* **Query Rewriting**

  * Follow-up questions rewritten into standalone queries
* **Evaluation & Analysis**

  * Baseline comparison (vector vs hybrid)
  * Ablation study (vector-only vs vector + graph)
* **Production-First Design**

  * Minimal runtime dependencies
  * Offline evaluation separated from deployment

---

## ğŸ§  System Architecture

```
User Query
   â”‚
   â”œâ”€â”€ Conversation Memory
   â”‚        â””â”€â”€ Query Rewriting
   â”‚
   â”œâ”€â”€ Hybrid Retrieval
   â”‚     â”œâ”€â”€ Vector Search
   â”‚     â”œâ”€â”€ Lexical Search
   â”‚     â””â”€â”€ Graph Expansion
   â”‚
   â”œâ”€â”€ Reranking
   â”‚
   â”œâ”€â”€ LLM Generation
   â”‚
   â””â”€â”€ Citation Filtering
           â””â”€â”€ Page-level evidence
```

---

## ğŸ” Retrieval Strategy

### Vector Search

* Sentence-transformer embeddings
* Semantic similarity search

### Lexical Search

* BM25 / keyword-based retrieval
* Improves recall for exact and technical terms

### Graph-Based Retrieval (AtlasRAG)

* Builds a **concept co-occurrence graph** during ingestion
* Expands retrieval via entity relationships
* Improves **coverage and diversity** without harming recall

This hybrid strategy balances:

* Semantic understanding
* Symbolic structure
* Explicit document grounding

---

## ğŸ“ Question Answering

* Answers are generated **only from retrieved context**
* If information is missing, the system responds:

  > *â€œI don't know based on the provided documents.â€*
* Citations are extracted **after generation**
* Citations include:

  * Page numbers
  * Supporting text snippets

---

## ğŸ“„ Document Summarization

* Uses **all indexed chunks**
* No top-k truncation
* Designed for long-form documents such as:

  * Research papers
  * Technical documentation
  * Academic PDFs

QA and summarization share the same backend pipeline.

---

## ğŸ§  Memory & Query Rewriting

### Conversation Memory

* Short-term, session-based
* Stores recent user and assistant turns

### Query Rewriting

* Converts follow-up questions into standalone queries
* Improves retrieval quality without polluting the retriever

Example:

```
User: What is self-attention?
User: Why is it better?
â†“
Rewritten Query:
Why is self-attention better than recurrence and convolution?
```

---

## ğŸ“Š Evaluation Methodology

Evaluation focuses on **retrieval quality**, not subjective LLM scoring.

### Metrics Used

* **Recall@K** â€” Whether relevant pages were retrieved
* **Coverage** â€” Number of unique relevant pages retrieved
* **Diversity** â€” Distribution of retrieved pages

### Experiments

* **Baseline Comparison**

  * Vector Search vs Hybrid Graph-RAG
* **Ablation Study**

  * Vector-only vs Vector + Graph expansion

All evaluation runs **offline** and is excluded from production deployment.

---

## âš™ï¸ Tech Stack

### Backend

* Python
* FastAPI
* Sentence Transformers
* Qdrant (Vector Store)
* NetworkX (graph reasoning)
* LangChain (optional integration)
* Groq / OpenAI-compatible LLM interface

### Frontend

* Deployed on Vercel
* Document upload + chat interface

### Tooling

* Ruff (linting)
* Pre-commit hooks
* Docker

---

## ğŸš€ Deployment Notes

* Backend deployed on **Hugging Face Spaces**
* Frontend deployed on **Vercel**
* Vector database initialized at runtime
* Uploaded documents stored dynamically (not committed to git)

---

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/           # LLM & prompts
â”‚   â”œâ”€â”€ ingestion/      # PDF parsing & chunking
â”‚   â”œâ”€â”€ retrieval/      # Vector, graph, hybrid search
â”‚   â”œâ”€â”€ memory/         # Conversation memory & rewriting
â”‚   â”œâ”€â”€ evaluation/     # Baseline & ablation analysis
â”‚   â””â”€â”€ api/            # FastAPI routes
```

---

## ğŸ¯ Design Rationale

* **Hybrid retrieval over pure vector search**
  â†’ Better robustness and coverage
* **Citation filtering post-generation**
  â†’ Prevents hallucinated references
* **Offline evaluation**
  â†’ Clean production runtime
* **Query rewriting instead of raw chat context**
  â†’ Improves retrieval precision
* **Minimal deployment dependencies**
  â†’ Faster builds, fewer failures

---

## ğŸ“„ License

[MIT License](LICENSE)